\documentclass[A4]{article}

%\usepackage[iso]{umlaute}
%\usepackage{german}
\usepackage{graphicx}
\setlength{\parindent}{0cm}
\setlength{\columnsep}{25pt}
\sloppy

% Your name
\author{Paul Preissner \\ Technische Universit\"at M\"unchen}

\title{Seminar Advanced Computer Architecture \\
       {\bf Actors}
}

% Date of your talk
\date{06.07.2017}


\begin{document}

\maketitle

\begin{abstract}
The actor model is a programming paradigm developed specifically with concurrent and distributed computing in mind. It defines a system of self-contained actors that only communicate with each other through dedicated messages and adhere to the basic semantic properties of encapsulation, fairness and location transparency. 
This paper will elaborate on the fundamental concepts and aforementioned semantics of the model, discuss the issues of implementation, current practical usage of the model and a brief comparison with other concurrency models. 
\end{abstract}

\section{Introduction}
\label{introduction}
Ever since the introduction of multi-processor systems in the 1970s and multi-core processors in the early 2000s, one of the main topics in computer science has been how these systems can be utilized to their full capacity, always dependent on the programming language employed. 
Most programming languages have been adapted to enable concurrent computing in some form, natively or through libraries. But since "[c]oncurrency can be solved by a good programmer in many languages, [...] but it's a tough problem to solve" as former Twitter engineer Alex Payne describes \cite[p.~15]{Agha2016:2}, it seems counter-intuitive to use a language originally defined for sequential execution. As such in the 1970s research first delved into defining languages and paradigms meant to primarily deal with concurrent computing. One of these paradigms is the actor model, initially developed by Carl Hewitt and his colleagues, formalized by Gul Agha in 1985 and researched by multiple groups and universities since then \cite{reference/parallel/KarmaniA11}. As per Payne again, the model is "commonly used to solve concurrency problems, and it makes that problem a lot easier to solve" \cite[p.~15]{Agha2016:2}.
Subsequently this paper will take a look at the fundamentals and basic semantics of the Actor model, followed by a discussion of some issues the model entails in implementation, concluded by a rundown of promising current usage scenarios and brief comparison to other models. 

\section{Fundamentals \& semantics of the Actor model}
\subsection{Fundamental concept}
As mentioned before, the model is based on the message-passing paradigm, which sees communication as the exchange of data/directive messages between whatever is defined as a computational unit. Computation is encapsulated into actors and while communication is only conducted via messages. As Agha puts it, the model furthers one key advantage of object-oriented programming, the separation of an object's interface from its representation, and expands it by separation of control from the computational logic \cite{reference/parallel/KarmaniA11}. This allows actor-based programs to be made up of "self-contained, autonomous, interactive, asynchronously operating components" \cite[p.~1]{reference/parallel/KarmaniA11} which makes them ideal for inherently nondeterministic systems such as distributed or mobile networks. 
\subsubsection{Actor}
As per its definition, a faithfully implemented actor can only send messages to actors it already knows. It is not allowed to guess or construct names or adresses of other actors \cite{reference/parallel/KarmaniA11}. Construction of a recipient's identifier would only be allowed if all properties and valid value ranges of the identifier are known. 
Furthermore, an actor by definition when receiving a message may proceed with any and all of three possible actions given it is idle and ready to compute: 
\begin{itemize}
\item The actor may send messages to other actors, for example as a reply to the original sender or as subsequent communication to other actors,
\item create new actors for arbitrary purpose, for example as workforce for a certain parallel task,
\item update its local state, in that it may do arbitrary computations within itself.
\end{itemize}
The third action leads to another fundamental restriction of the Actor model, an actor can only influence its own local state . It is not allowed to directly change the state of other actors in any way, such changes have to be requested through messages.\cite{reference/parallel/KarmaniA11} This presents a common pitfall where a faithful implementation tends to prove inefficient and is willingly omitted in favor of better performance, introducing hazards in execution. 
\subsubsection{Sample actor program}
To illustrate actors, the C++ Actor Framework \cite{cshw-nassp-13}\cite{chs-rapc-16} was used to implement a small Ping Pong program as in listing 1. An Actor A sends a string "Ping" to an Actor B. Actor B replies to any incoming messages matching to "Ping" with a reply "Pong". Actor A reacts to any incoming "Pong" with yet another "Ping".
The output of this program is simply a continuous back and forth of "Ping" and "Pong". It does show, however, that using actors in programs proves fairly easily given a properly implemented library. Note that no thread management or balancing has to be done by the high level programmer. It is only necessary to specify the behaviors for actors and kickstart a first call on at least one actor. The rest is done almost entirely by the library and this small sample program is scheduled by CAF to utilize all four threads of the executing Intel Core i7-7500U to at least 75\% utilization. 

\subsection{Semantics}
The Actor model by definition has a few major semantic properties. These are "encapsulation and atomic execution of methods (where a method represents computation in response to a message), fairness, and location transparency" \cite[p.~4]{reference/parallel/KarmaniA11} which if enforced almost guarantee correct execution of the model, if not, however, put the task of checking for faithful implementation onto the programmer. 
\subsubsection{Encapsulation and atomic execution}
Encapsulation describes the previously mentioned property that no actor can directly influence the state of another actor. More precisely, "no two actors share state" \cite[p.~4]{reference/parallel/KarmaniA11} and on a conceptual level change state atomically. An actor upon receiving a message will compute based on the behavior defined within and will not branch into different computations if it receives another message while busy. Faithfully the actor will buffer the pending message, finish its current computation, then continue with the pending message, effectively completing an atomic step. 
While in theory for the sake of performance it makes sense to interrupt current computation and switch to a different task the moment a message arrives or start work in parallel, this violates the semantic definition of the Actor model, introduces hazards and compounds modeling and checking of the program. 
\subsubsection{Fairness}
In Karmani's paper, fairness is the property that "every actor makes progress if it has some computation to do, and [...] every message is eventually delivered [...] unless the destination actor is permanently disabled" \cite[p.~5]{reference/parallel/KarmaniA11}. Due to the previously explained atomicity, an actor will always compute something if it received a message. 
This notion of fairness assumes that the underlying scheduler is fair as well. 
Given that, one can reason more easily about the liveness properties of a program \cite[p.~5]{reference/parallel/KarmaniA11} as this property guarantees that in a composition of actor systems, no busy system can inhibit the progress of another system thus stall the entire program.
\subsubsection{Location transparency}
One more property of the model is that the actual location of an actor is independent of its identifier. A program can be constructed without the need to know the physical topology of the executing system. This facilitates another property, mobility.
As Karmani puts it, "Mobility is defined as the ability of a computation to migrate across different nodes" \cite[p.~5]{reference/parallel/KarmaniA11}, meaning actors can be shifted across nodes without affecting the correctness of execution. This is key for effective runtime load-balancing of irregular and dynamic applications, energy efficiency optimization, fault-tolerance in case of partial network failure and improved scalability. Scalability is still one of the primary question marks for many concurrent computing models and languages as distributed systems keep growing both in number of computing units and physical distance covered, and single computers in terms of cores, co-processors and sensors that are integrated into a single composition. 

\subsection{Means of synchronization and abstraction}
\subsubsection{Synchronization}
Synchronization in the actor model  has to be handled through messages. Karmani describes a solution based on the same principle as traditional procedure calls, "Remote Procedure Call (RPC)-like messaging" \cite[p.~6]{reference/parallel/KarmaniA11}. A request is sent out by an actor A which then waits. If it receives a matching reply by any actor B, A proceeds. If not, B's message is deferred until the expected reply arrives \cite[p.~6]{reference/parallel/KarmaniA11}. Specific implementation is down to the individual library or language.
RPC-like messages offer themselves as a solution for ordered processing of messages or when all next actions of the requestor depend on the reply. 
Another approach are so-called local synchronization constraints. A set of constraints is defined on an actor that specifies the order it should keep and process messages in, depending on its state and the message type and content. \cite[p.~34]{Agha2016:2}
Alternatively, specialized constructs called Synchronizers can be used to act as a sort of pre-processing of messages that ensures correct order.
Karmani suggests that these approaches can be used to construct patterns like pipelining and divide-and-conquer to map actors to a more complex task distribution in a modular fashion \cite[p.~6]{reference/parallel/KarmaniA11}.
\subsubsection{Abstraction}
Patterns may also be used in a different fashion for abstraction. As Karmani points out, ideally a programmer can utilize a language or library in a fashion similar to the conceptual level of the problem at hand. Instead of treating actors as single actors one would prefer to have abstractions that compose actors in a useful way. \cite[p.~7]{reference/parallel/KarmaniA11}
Patterns can be employed to structure communication, in that groups of actors are assigned properties by which they can be identified or discarded as recipients for certain messages. For example actors can specify their group and that they are only interested in specific message data. A sender can specify properties the recipients of its message need to satisfy. A message can then be sent out to a satisfying group without performing many individual checks. 
Other abstractions and sychronization techniques are certainly possible but might be more specifically tailored for the respective problem or application field of the library or language.

\section{Reality and issues of implementation}
While these semantics are logical in theory, the reality of implementation is different. Not all properties can be implemented faithfully while still performing well without major optimization work. This will inevitably lead programmers attempting to implement an actor language or library to take shortcuts to achieve better performance. But those supposed optimizations might violate the properties and constraints leading to flawed execution. Karmani notes that "a faithful but naive implementation of the Actor model can be highly inefficient" \cite[p.~8]{reference/parallel/KarmaniA11} and may only be feasible for coarse-grained concurrency \cite[p.~7]{conf/pppj/KarmaniSA09} but this "may be adressed by compilation and runtime techniques" \cite[p.~8]{reference/parallel/KarmaniA11}. This chapter will outline a few potential pitfalls and potential solutions.
\subsubsection{Message latency}
Basically natural to any system (especially distributed) is latency. The further a message has to travel, the higher the latency between send and receive is. 
Conceptually this is fixed by overlapping communication and computation. This way computation can partially mask the latency. This can still be outweighed by a disproportional surplus of messages, so a program needs to be engineered as to not overburden communication. Good overlap also needs suitable decomposition of the program into actors and smart migration. \cite[p.~8]{reference/parallel/KarmaniA11}
Another more crude aspect that may lower latencies is improved network interconnects, be it connections between distributed clients or data buses in embedded systems. 
\subsubsection{Naive send vs channels}
It was previously established that actors may only send messages to actors they know. Some languages and libraries modify this idea towards the concept of channels, specifically with stateful channel contracts, "a protocol that governs the communication between two end-points (actors) of the channel" \cite[p.~8]{reference/parallel/KarmaniA11}. This way the allowed type and order of messages can be specified in a more intuitive way as opposed to individual checks of messages in each actor and additional synchronization. \cite[p.~8]{reference/parallel/KarmaniA11}
\subsubsection{Garbage collection}
Another topic Karmani briefly mentions is garbage collection, which as of yet is an unresolved issue in distributed systems as it is not yet solved how to efficiently check for reachability across the entire system. \cite[p.~8]{reference/parallel/KarmaniA11}
\subsubsection{Thread overhead and context switches}
A basic actor language or library might opt for mapping an actor to a thread for simplicity. The issue this presents is overhead due to context switching and thread creation. Every time the program execution switches to a different actor it has to perform a complete context switch including saving its entire stack, counters and registers. This problem becomes increasingly prevalent the more actors a program uses. Karmani presents one possible solution in continuations based actors which don't perform a full context switch and significantly reduce overhead. In his testing, going from traditional threads to continuations in ActorFoundry reduced runtime of a thread switching benchmark by over 60\% from 695s to 267s. \cite[p.~8]{conf/pppj/KarmaniSA09}
\subsubsection{Deep copies vs referencing}
By definition, an actor may not change the state of another actor directly. Naively on a shared memory system any data exchange would need to happen by copying the data into a message, which is slow in contrast to sending a reference. Sending a reference however carries risk of direct changes violating the model. Karmani's optimization efforts suggest allowing immutable types to be sent by reference. According to his numbers in the aforementioned benchmark, sending the main message content type by reference reduced runtime by another 84\%. \cite[p.~9]{conf/pppj/KarmaniSA09}
The speedup achieved this way depends on which message content types are most common in a program as well as the scale of the system.
\subsubsection{Scheduling}
The model claims fairness in that every actor will make progress if it has pending messages. This, however, has to be ensured by the scheduler. It's not inherently an issue but can be if the scheduler does not comply. What Karmani did for his testing was to write a custom scheduler which monitors the worker threads and spawns a new thread when no progress is made yet actors are queuing to be scheduled. While not highly sophisticated, his method when tweaked performs almost identical to the default JVM scheduler but does guarantee fairness. \cite[p.~9]{conf/pppj/KarmaniSA09}
\subsubsection{Safe messaging}
In the discussion of his paper, Karmani mentions safe messaging as a remaining major topic for optimization. This goes back to deep copying versus referencing message content. It remains an active research topics how it could be reliably and efficiently determined which types may be safely sent by reference.\cite[p.~10]{conf/pppj/KarmaniSA09}

\subsection{Tools and languages}
Tool support for actor systems is still rather basic. There are some plug-ins for Erlang and Scala for Eclipse for example and some testing tools for Erlang or Java actors exist, but widespread support is still an active development topic. \cite[p.~10]{reference/parallel/KarmaniA11}
Libraries exist for many languages and plenty languages have been designed around the actor model, many still actively supported. Notable mentions include Erlang, Scala and SALSA along with many others. Libraries exist for most common major languages. An issue that can be seen with some of these libraries is that while they are actively updated, they have lackluster documentation and little developer support. It is noteworthy though that wide ground is covered in terms of the system structures supported, be it distributed or embedded systems, small scale or large scale. \cite{WikiLang}

\section{Current usage of actor systems}
\subsubsection{Actors in distributed systems}
There are multiple levels at which the model can be applied in distributed systems. An intuitive option is to model clients as actors and map them 1:1 or cluster several clients into an actor. Communication then maps to the physical links between clients. This may not be efficient or suitable to the application though. A more useful mapping is to decouple and map parts of the program, such as a user of a service or a certain part of a service itself. This is the case for systems such as the Facebook chat backend \cite{Facebook}, Twitter message queues \cite{Twitter}, game servers for the video game Halo 4 \cite{Halo4} or the web application framework Lift \cite[p.~14]{Agha2016:2}. Other projects exist in high performance computing, demonstrated for example by the ActorX10 group of the Invasic project at TUM which uses a fusion of X10 and the actor model to illustrate how actors can be used to build highly scalable programs while also simplifying communication and separating control flow from computation \cite[p.~4]{Roloff:2016:AAL:2931028.2931033}. 
A similar potential lies in so-called microservices, a type of service-oriented architecture where collections of independent services are composed into a whole application. The idea is to achieve better scalability, decentralized control and fault-tolerance. \cite{MicroS}
\subsubsection{Actors in embedded systems}
The model is also suitable to map programs and algorithms on a very small scale in embedded systems. About actor-oriented design of embedded systems in 2002, Edward A. Lee et al. argue this approach is "particularly effective for system-level design" \cite[p.~12]{Lee03actor-orienteddesign}. Components of the system, such as parts of an algorithm, are modeled as actors to enable better reasoning over execution within the system. The actor model offers them a clear method for abstraction and definition of interfaces, object representations and connections. Lee et al. state that "[t]he primary benefit of actor-oriented design is the possibility of succinctly capturing the requirements of an embedded system by the modeling properties of a model of computation" \cite[p.~23]{Lee03actor-orienteddesign}, specifically satisfying  the requirements of model-based design. 
Contrary to using actors in the design stage, some efforts use the model directly in the implementation of programs. The previously mentioned ActorX10 library illustrates this by implementing an object detection module using an algorithm that features multiple stages. Each stage is modeled as an actor receiving data messages from the previous stage actor and sending its result to the next. This structure can be exploited for parallelism by pipelining. Once a stage is finished with one input, it can immediately work on the next input data. Every stage can have almost 100\% uptime and a final result is had each time the last stage finishes. \cite[p.~4]{Roloff:2016:AAL:2931028.2931033} This could allow even further optimization, for example in hardware through specialized ASICs for certain stages. 
Omer Kilic of the Erlang Solutions group also sees the actor model very suited for current challenges in computing technology such as the creation of complex SoCs, IoT devices and heterogenous architectures \cite[p.~5]{ErlangKilic}.

\section{Versus other models of concurrency}
\subsubsection{vs Petri nets}
Petri nets on a conceptual level are mathematical graph-based models with two types of nodes, places and transitions. Places are equal to conditions of any form while transitions are possible events. Flow within the model is represented by arcs pointing from either a place or a transition to the respective other. The net describes a complex system of pre- and postconditions for possible events. Every such place/condition can have an arbitrary number of tokens which can each trigger a transition and are then "transferred" to the postcondition of that event. \cite{Petri1}\cite{Petri2}\cite{murata1989petri}
This can be exploited for concurrency by placing multiple tokens in the net. 
The largest difference to the actor model appears to be that actors are modeled as the (abstract) computing units themselves with communication between each which can lead to state changes while a petri net describes the possible changes and events mostly independent of the representation of computation.
\subsubsection{vs process calculi}
Process calculi are not one model specifically but a collection of similar formal models. Their basic definitions all include a few key properties. Processes communicate/interact through message-passing, similar to the actor model, however anonymously via channels, unlike the pure actor model. Processes are modeled as a composition of primitives and operators, unlike the actor model which does not specifically model the actions within an actor. These operators are subject to algebraic laws to facilitate reasoning about the system. \cite{Baeten:2005:BHP:1085667.1085669} The partial similarities between the two models do not come as a surprise considering Carl Hewitt named process calculi as one influence on his work. 
\subsubsection{vs input/output automaton}
I/O automata approach the issue of concurrency at an even more modular level. They can be used to model single components of almost any kind, be it computational units, data or communication methods. An automaton is essentially a state machine with inputs, outputs and some sort of internal action. Only the in- and outputs may be used to communicate with other automatons, internals are hidden. 
In this way there is some resemblance to the encapsulation in the actor model, however the actor model does already specify communication and basic actor structure. The automata could in theory be used to model an actor system, however with more effort.
\section{Conclusion}
\label{conclusion}
Conclusion.

% Put citations from bibtex into References section which were not
% explicity cited.
\nocite{robotron,
stonx,vice,650sim,herculessim,zib,4004,thermal1,thermal2,rojas}


\bibliographystyle{plain}
\bibliography{seminarpaper}
\end{document}